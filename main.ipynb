{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a833a37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import json\n",
    "import os\n",
    "import uuid\n",
    "import sys\n",
    "\n",
    "import warnings\n",
    "import gzip\n",
    "from Bio.PDB.PDBExceptions import PDBConstructionWarning\n",
    "from Bio.PDB import PDBParser\n",
    "from Bio.SeqUtils import seq1\n",
    "warnings.simplefilter('ignore', PDBConstructionWarning)\n",
    "\n",
    "import torch\n",
    "import torch as ch\n",
    "import torch.nn as nn\n",
    "from fastargs import Param, Section\n",
    "from fastargs.validation import And, OneOf\n",
    "import numpy as np\n",
    "import src.config_parse_utils as config_parse_utils\n",
    "from src.eval_utils import evaluate_model\n",
    "from src.trainer import LightWeightTrainer\n",
    "from src.models_and_optimizers import create_clip_model, load_model\n",
    "import src.dist_utils as dist_utils\n",
    "import src.data_utils as data_utils\n",
    "from transformers import EsmTokenizer\n",
    "import src.loader as loaders_utils\n",
    "import webdataset as wds\n",
    "import tqdm\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import logging\n",
    "from functools import partial\n",
    "import src.loader as loader_utils\n",
    "import src.zipdataset as zipdataset_utils\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # FATAL\n",
    "logging.getLogger('tensorflow').setLevel(logging.FATAL)\n",
    "\n",
    "import src.models_and_optimizers as model_utils\n",
    "from types import SimpleNamespace\n",
    "from clip_main import get_wds_loaders\n",
    "from transformers import EsmTokenizer\n",
    "import src.data_utils as data_utils\n",
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch.cuda.amp import autocast\n",
    "from transformers import EsmTokenizer, EsmModel\n",
    "import esm as esmlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d787a67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## WDS helpers\n",
    "\n",
    "def process_residue(residue):\n",
    "    atoms = ['N', 'CA', 'C', 'O']\n",
    "    coordinates = []\n",
    "    for r in atoms:\n",
    "        coord = residue.child_dict.get(r, None)\n",
    "        if coord is None:\n",
    "            if r == 'O':\n",
    "                coord = residue.child_dict.get('OXT', None)\n",
    "            if coord is None:\n",
    "                return None, None\n",
    "        coordinates.append(np.array(coord.get_coord()))\n",
    "    return np.stack(coordinates), seq1(residue.resname)\n",
    "\n",
    "def process_chain(chain):\n",
    "    coordinates = []\n",
    "    seq = []\n",
    "    for r in chain:\n",
    "        output, residue_name = process_residue(r)\n",
    "        if output is not None:\n",
    "            coordinates.append(output)\n",
    "            seq.append(residue_name)\n",
    "    if len(coordinates) == 0:\n",
    "        return None\n",
    "    coordinates = np.stack(coordinates)\n",
    "    seq = ''.join(seq)\n",
    "    return coordinates, seq\n",
    "\n",
    "def process_chains(chains, pep=False, prot=False):\n",
    "    if pep or prot:\n",
    "        chain_lens = []\n",
    "        chain_ids = []\n",
    "        for chain in chains:\n",
    "            for i, res in enumerate(chain):\n",
    "                continue\n",
    "            chain_lens.append(i)\n",
    "            chain_ids.append(chain.id)\n",
    "        if chain_lens[0] < chain_lens[1]:\n",
    "            pep_id = chain_ids[0]\n",
    "            prot_id = chain_ids[1]\n",
    "        else:\n",
    "            pep_id = chain_ids[1]\n",
    "            prot_id = chain_ids[0]\n",
    "        if pep and isinstance(pep, str): pep_id == pep\n",
    "        if prot and isinstance(prot, str): prot_id == prot\n",
    "    output = []\n",
    "    chain_ids = []\n",
    "    for chain in chains:\n",
    "        if (pep and chain.id != pep_id) or (prot and chain.id != prot_id):\n",
    "            continue\n",
    "        out = process_chain(chain)\n",
    "        if out is not None:\n",
    "            output.append(out)\n",
    "            chain_ids.append(chain.id)\n",
    "    coords = [u[0] for u in output]\n",
    "    seqs = [u[1] for u in output]\n",
    "    return coords, seqs, chain_ids\n",
    "\n",
    "def process_structure(structure, pep=False, prot=False):\n",
    "    for s in structure: # only one structure\n",
    "        return process_chains(s, pep, prot)\n",
    "    return None\n",
    "\n",
    "# +\n",
    "def process_pdb(parser, pdb_filename):\n",
    "    # print(pdb_filename)\n",
    "    with gzip.open(pdb_filename, \"rt\") as file_handle:\n",
    "        structure = parser.get_structure(\"?\", file_handle)\n",
    "        date = structure.header['deposition_date']\n",
    "        return process_structure(structure), date\n",
    "    \n",
    "def process_pdb_raw(parser, pdb_filename, pep=False, prot=False):\n",
    "    s = parser.get_structure(\"?\", pdb_filename)\n",
    "    return process_structure(s, pep, prot)\n",
    "\n",
    "def read_input_ids(index_file):\n",
    "    input_ids = []\n",
    "    with open(os.path.join(index_file), 'r') as f:\n",
    "        for line in f:\n",
    "            input_ids += [line.strip()]\n",
    "    return np.array(input_ids)\n",
    "\n",
    "def write_dataset(dataset, tar_name, use_shards=False, max_shard_count=10000):\n",
    "    if use_shards:\n",
    "        os.makedirs(tar_name, exist_ok=True)\n",
    "        sink = wds.ShardWriter(f'{tar_name}/shard-%06d.tar',maxcount=max_shard_count)\n",
    "    else:\n",
    "        sink = wds.TarWriter(tar_name)\n",
    "    for index, (batch, pdb_id) in enumerate(dataset):\n",
    "        if index%1000==0:\n",
    "            print(f\"{index:6d}\", end=\"\\r\", flush=True, file=sys.stderr)\n",
    "        if len(batch[0]) == 0:\n",
    "            continue\n",
    "        sink.write({\n",
    "            \"__key__\": \"sample%06d\" % index,\n",
    "            \"inp.pyd\": dict(coords=batch[0], seqs=batch[1], chain_ids=batch[2], pdb_id=pdb_id),\n",
    "        })\n",
    "    sink.close()\n",
    "    \n",
    "def make_wds(dir_, tar_):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        dir_ (str): Directory containing PDB files.\n",
    "        tar_ (str): Output file path to write WDS to.\n",
    "    \"\"\"\n",
    "    parser = PDBParser()\n",
    "    root_pdb = dir_\n",
    "    outputs = []\n",
    "    for i, pdb_file in tqdm.tqdm(enumerate(os.listdir(dir_)), total=len(os.listdir(dir_))):\n",
    "        pdb_file = pdb_file.strip()\n",
    "        pdb_file = os.path.join(dir_, pdb_file)\n",
    "        out = process_pdb_raw(parser, pdb_file)\n",
    "        pdb_id = pdb_file.split('.')[0]\n",
    "        for sequence in \n",
    "        outputs.append((out, pdb_id))\n",
    "\n",
    "    dataset = []\n",
    "    for o, pdb_id in tqdm.tqdm(outputs):\n",
    "        if o is None:\n",
    "            continue\n",
    "        dataset.append((o, pdb_id))\n",
    "\n",
    "    write_dataset(dataset, tar_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb44bc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## GENERAL SETUP (CHANGE PATHS AS NEEDED)\n",
    "ROOT = \"/home/gridsan/lguan/keating/rla/model_weights\"\n",
    "model_dir = \"version_0/\" \n",
    "dev = 'cuda:0'\n",
    "CLIP_MODE = False\n",
    "root_path = os.path.join(ROOT, model_dir)\n",
    "path = os.path.join(root_path, \"checkpoints/checkpoint_best.pt\")\n",
    "data_root = \"/home/gridsan/lguan/keating/pacap/rfdiffusion/20240925/pacap_bind/outputs/wds\"\n",
    "args_path = os.path.join(ROOT, model_dir, [u for u in os.listdir(os.path.join(ROOT, model_dir)) if u.endswith('.pt')][0])\n",
    "\n",
    "backwards_compat = {\n",
    "    'masked_rate': -1,\n",
    "    'masked_mode': 'MASK',\n",
    "    'lm_only_text': 1,\n",
    "    'lm_weight': 1,\n",
    "    'resid_weight': 1,\n",
    "    'language_head': False,\n",
    "    'language_head_type': 'MLP',\n",
    "    'zip_enabled': False,\n",
    "    'num_mutations': False,\n",
    "}\n",
    "hparams = torch.load(args_path)\n",
    "args_dict = hparams['args']\n",
    "args_dict['data_root'] = data_root\n",
    "args_dict['batch_size'] = 1\n",
    "args_dict['blacklist_file'] = ''\n",
    "args_dict['num_workers'] = 1\n",
    "for k in backwards_compat.keys():\n",
    "    if k not in args_dict:\n",
    "        args_dict[k] = backwards_compat[k]\n",
    "args = SimpleNamespace(**args_dict)\n",
    "\n",
    "print(vars(args))\n",
    "\n",
    "coordinator_params = data_utils.get_coordinator_params(args.coordinator_hparams)\n",
    "coordinator_params['num_positional_embeddings'] = args.gnn_num_pos_embs\n",
    "coordinator_params['zero_out_pos_embs']= args.gnn_zero_out_pos_embs\n",
    "coordinator_params['clip_mode'] = True"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
